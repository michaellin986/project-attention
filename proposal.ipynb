{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGMAhIAC1k6i"
   },
   "source": [
    "# CS 449 Final Project Proposal\n",
    "\n",
    "Due: April 21, 2023 at 11:59pm\n",
    "\n",
    "## 1. Names and Net IDs\n",
    "\n",
    "*List your group members*\n",
    "\n",
    "## 2. Abstract\n",
    "\n",
    "*Your abstract should be two or three sentences describing the motivation\n",
    "for your project and your proposed methods.*\n",
    "\n",
    "> For example:\n",
    "> \n",
    "> Our final project seeks to use a variety of `sklearn` models to classify\n",
    "> handwritten digits in the `MNIST` dataset. We will compare models such\n",
    "> as Logistic Regression and Multilayer Perceptrons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YX2wBFIs2S9Q"
   },
   "source": [
    "## 3. Introduction\n",
    "\n",
    "*Why is this project interesting to you? Describe the motivation for pursuing this project. Give a specific description of your data and what machine learning task you will focus on.*\n",
    "\n",
    ">For example:\n",
    "> \n",
    "> It is very important for us to be able to automatically recognize handwritten digits so that the Postal Service can identify whether a letter has been sent to the correct address. We will use a large dataset of handwritten digits and train our models to input the black-and-white pixels of those digits and output the number that was written. [etc. etc.]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B-1Lwrn635Qa"
   },
   "source": [
    "## 4a. Describe your dataset(s)\n",
    "\n",
    "*List the datasets you plan to use, where you found them, and what they contain. Be detailed! For each dataset, what does the data look like? What is the data representation? (e.g., what resolution of images? what length of sequences?) How is the data annotated or labeled? Include citations for the datasets. Include at least one citation of previous work that has used your data, or explain why no one has used your data before.*\n",
    "\n",
    "> We will be using the WMT (Workshop on Statistical Machine Translation) 2014 English-German and English-French datasets. These datasets provide examples of phrases in English and its translation in German or French (and vice versa). Specific details about the datasets can be found here: https://aclanthology.org/W14-3302.pdf . In summary, the datasets come from formal sources (European Parliment, United Nations, news sources, etc.). They directly translated text from the source language to the target language (i.e. with no intermediary language), using machine translation, and this translation subsequently was followed by an involved manual evaluation process. The dataset does not involve any tokenization; it is purely the source text and its translation. Each example is relatively short: as stated before, they are either a simple phrase or a sentence. The reason for using this dataset is to replicate the work of Vaswani et al. in their influential paper \"Attention is All you Need.\"\n",
    "\n",
    "> @InProceedings{bojar-EtAl:2014:W14-33,\n",
    "  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale\n",
    "{s}},\n",
    "  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},\n",
    "  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},\n",
    "  month     = {June},\n",
    "  year      = {2014},\n",
    "  address   = {Baltimore, Maryland, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {12--58},\n",
    "  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}\n",
    "}\n",
    "\n",
    ">@inproceedings{NIPS2017_3f5ee243,\n",
    " author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \\L ukasz and Polosukhin, Illia},\n",
    " booktitle = {Advances in Neural Information Processing Systems},\n",
    " editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n",
    " pages = {},\n",
    " publisher = {Curran Associates, Inc.},\n",
    " title = {Attention is All you Need},\n",
    " url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},\n",
    " volume = {30},\n",
    " year = {2017}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Load your dataset(s)\n",
    "\n",
    "*Demonstrate that you have made at least some progress with getting your\n",
    "dataset ready to use. Load at least a few examples and visualize them\n",
    "as best you can*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "VUfetuVm5WTy",
    "outputId": "30b7a0f8-9bb8-4ad9-9376-55fed29f23dd"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "'''\n",
    "    streaming=True is important. It will otherwise download the whole dataset. It will probably take an hour to load both of these in this way.\n",
    "\n",
    "    Quick Guide:\n",
    "        IterableDatasetDict : https://huggingface.co/docs/datasets/v2.11.0/en/package_reference/main_classes#datasets.IterableDatasetDict\n",
    "        IterableDataset     : https://huggingface.co/docs/datasets/v2.11.0/en/package_reference/main_classes#datasets.IterableDataset\n",
    "        \n",
    "        `load_dataset(..., streaming=True)` returns an `IterableDatasetDict`.\n",
    "        Use 'train', 'test', or 'validation' as keys to access the respective data of type `IterableDataset`.\n",
    "        On `IterableDataset`,\n",
    "            use `take(n)` for some n:int > 0 to get `IterableDataset` with the first n examples.\n",
    "            use `shuffle()` to shuffle the dataset\n",
    "'''\n",
    "dataset_fr = load_dataset(\"wmt14\", \"fr-en\", streaming=True)\n",
    "dataset_de = load_dataset(\"wmt14\", \"de-en\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'fr': 'Reprise de la session', 'en': 'Resumption of the session'}}\n",
      "{'translation': {'fr': 'Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.', 'en': 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.'}}\n",
      "\n",
      "{'translation': {'fr': 'Spectaculaire saut en \"wingsuit\" au-dessus de Bogota', 'en': 'Spectacular Wingsuit Jump Over Bogota'}}\n",
      "{'translation': {'fr': \"Le sportif Jhonathan Florez a sauté jeudi d'un hélicoptère au-dessus de Bogota, la capitale colombienne.\", 'en': 'Sportsman Jhonathan Florez jumped from a helicopter above Bogota, the capital of Colombia, on Thursday.'}}\n",
      "\n",
      "{'translation': {'fr': \"Une stratégie républicaine pour contrer la réélection d'Obama\", 'en': 'A Republican strategy to counter the re-election of Obama'}}\n",
      "{'translation': {'fr': 'Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.', 'en': 'Republican leaders justified their policy by the need to combat electoral fraud.'}}\n",
      "\n",
      "{'translation': {'de': 'Wiederaufnahme der Sitzungsperiode', 'en': 'Resumption of the session'}}\n",
      "{'translation': {'de': 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.', 'en': 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.'}}\n",
      "\n",
      "{'translation': {'de': 'Gutach: Noch mehr Sicherheit für Fußgänger', 'en': 'Gutach: Increased safety for pedestrians'}}\n",
      "{'translation': {'de': 'Sie stehen keine 100 Meter voneinander entfernt: Am Dienstag ist in Gutach die neue B 33-Fußgängerampel am Dorfparkplatz in Betrieb genommen worden - in Sichtweite der älteren Rathausampel.', 'en': 'They are not even 100 metres apart: On Tuesday, the new B 33 pedestrian lights in Dorfparkplatz in Gutach became operational - within view of the existing Town Hall traffic lights.'}}\n",
      "\n",
      "{'translation': {'de': 'Eine republikanische Strategie, um der Wiederwahl von Obama entgegenzutreten', 'en': 'A Republican strategy to counter the re-election of Obama'}}\n",
      "{'translation': {'de': 'Die Führungskräfte der Republikaner rechtfertigen ihre Politik mit der Notwendigkeit, den Wahlbetrug zu bekämpfen.', 'en': 'Republican leaders justified their policy by the need to combat electoral fraud.'}}\n"
     ]
    }
   ],
   "source": [
    "for data in dataset_fr['train'].take(2):\n",
    "    print(data)\n",
    "print()\n",
    "for data in dataset_fr['test'].take(2):\n",
    "    print(data)\n",
    "print()\n",
    "for data in dataset_fr['validation'].take(2):\n",
    "    print(data)\n",
    "print()\n",
    "for data in dataset_de['train'].take(2):\n",
    "    print(data)\n",
    "print()\n",
    "for data in dataset_de['test'].take(2):\n",
    "    print(data)\n",
    "print()\n",
    "for data in dataset_de['validation'].take(2):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation': {'fr': 'Une boîte noire dans votre voiture\\xa0?', 'en': 'A black box in your car?'}}]\n",
      "Une boîte noire dans votre voiture ?\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    NOTE: When viewing some examples, it may appear to have some potential issues as seen below in \"voiture\\xa0?\".\n",
    "    However, this is simply a matter of how it is printed.\n",
    "    As seen in the output of this code block, this will work properly when directly observed.\n",
    "    \n",
    "    But also note in this example that there are going to be some inconsistencies in the data.\n",
    "    The \\xa0 is present in the French translation but not the English one, and this results in a space between the word and the question mark.\n",
    "'''\n",
    "printing_issue_example = list(dataset_fr['test'].skip(3).take(1))\n",
    "print(printing_issue_example)\n",
    "print(printing_issue_example[0]['translation']['fr'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c. Small dataset\n",
    "\n",
    "*Many deep learning datasets are very large, which is helpful for training powerful models but makes debugging difficult. For your update, you will need to construct a small version of your dataset that contains 200-1000 examples and is less than 10MB. If you are working with images, video, or audio, you may need to downsample your data. If you are working with text, you may need to truncate or otherwise preprocess your data.*\n",
    "\n",
    "*Give a specific plan for how you will create a small version of one dataset you'll use that is less than 10MB in size. Mention the current size of your dataset and how many examples it has and how those numbers inform your plan.*\n",
    "\n",
    "> By specifying `streaming=True` when initializing the dataset, it returns a version of the dataset that allows for immediate usage of the dataset without having to download the entire thing. This makes it so that it does not require any significant space as the data is streamed. Thus, making our dataset less than 10MB is trivial. See the below code block to see how much the streamed dataset takes up. Constructing a smaller version of our dataset is also trivial: simply specify the number of examples desired in the `take` function. This streamed dataset does not allow to see how many examples there are, but given the capabilities of this `IterableDataset`, this does not seem like this will cause an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of dataset_fr\t\t\t type: IterableDatasetDict\t 208 Bytes\n",
      "size of dataset_fr['train']\t\t type: IterableDataset:\t\t 56 Bytes\n",
      "size of dataset_fr['train'].take(1000)\t type: IterableDataset\t\t 56 Bytes\n"
     ]
    }
   ],
   "source": [
    "from sys import getsizeof # returns size in bytes\n",
    "\n",
    "print(\"size of dataset_fr\\t\\t\\t\", \"type: IterableDatasetDict\\t\", getsizeof(dataset_fr), \"Bytes\")\n",
    "print(\"size of dataset_fr['train']\\t\\t\", \"type: IterableDataset:\\t\\t\", getsizeof(dataset_fr['train']), \"Bytes\")\n",
    "print(\"size of dataset_fr['train'].take(1000)\\t\", \"type: IterableDataset\\t\\t\", getsizeof(dataset_fr['train'].take(1000)), \"Bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCKtudoJ6V4g"
   },
   "source": [
    "## 5. Methods\n",
    "\n",
    "*Describe what methods you plan to use. This is a deep learning class, so you should use deep learning methods. Cite at least one or two relevant papers. What model architectures or pretrained models will you use? What loss function(s) will you use and why? How will you evaluate or visualize your model's performance?*\n",
    "\n",
    "> For example:\n",
    "> \n",
    "> This is a standard supervised learning task, and we will use `sklearn`'s Logistic Regression model to predict digit labels from their pixels. The model will contain one weight per pixel. We will train our model using Cross-Entropy loss, because..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA5eIKsR7QRk"
   },
   "source": [
    "## 6. Deliverables\n",
    "\n",
    "*Include at least six goals that you would like to focus on over the course of the quarter. These should be nontrivial, but you should have at least one and hopefully both of your \"Essential\" goals done by the project update, due in mid-May. Your \"Stretch\" goals should be ambitious enough such that completing one is doable, but completing both this quarter is unlikely.*\n",
    "\n",
    "### 6.1 Essential Goals\n",
    "- (At least two goals here. At least one should involve getting a neural network model running.)\n",
    "\n",
    "> For example:\n",
    ">\n",
    "> We will use a Logistic Regression and a Multilayer Perceptron to train and test on our MNIST data.\n",
    "\n",
    "### 6.2 Desired Goals\n",
    "- (At least two goals here. Completing these goals should be sufficient for you to say your project was a success.)\n",
    "\n",
    "> For example:\n",
    ">\n",
    "> We will compare our MLP model against a pretrained Visual Transformer model that we fine-tune for this task.\n",
    "\n",
    "### 6.3 Stretch Goals\n",
    "- (At least two goals here. These should be ambitious extensions to your desired goals. You can still get full points without completing these.)\n",
    "> For example:\n",
    "> \n",
    "> We will conduct a manual analysis of the digits that our model gets wrong and use a GAN to create new images that help us learn a more robust classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlB_wLS381Xy"
   },
   "source": [
    "## 7. Hopes and Concerns\n",
    "\n",
    "*What are you most excited about with this project? What parts, if any, are you nervous about? For example:*\n",
    "\n",
    "> For example: \n",
    "> \n",
    "> We're worried that we'll get really bored of staring at pixelated hand-written digits for hours on end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2peFc_M8-E7"
   },
   "source": [
    "## 8. References\n",
    "\n",
    "*Cite the papers or sources that you used to discover your datasets and/or models, if you didn't include the citation above.*\n",
    "\n",
    "> For example:\n",
    "> \n",
    "> LeCun, Yann, et al. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE 86.11 (1998): 2278-2324."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
