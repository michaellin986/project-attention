{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Names and Net IDs\n",
        "\n",
        "- Kyle Hwang (ksh6947)\n",
        "- Michael Lin (qlb968)\n",
        "- Dylan Wu (dwg0364)\n",
        "\n",
        "# Abstract\n",
        "\n",
        "Our final project seeks to use `pytorch` to replicate the \"Attention is All You Need\" paper, which introduced the Transformer as a way to improve upon existing sequence transduction language models. We will attempt to implement the model's architecture and train the model on a subset of the WMT 2014 English-French dataset. We will then perform analyses on the training results, and, where appropriate, compare the model performance against results from the paper.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Goals and Discussion\n",
        "\n",
        "## Essential Goals\n",
        "\n",
        "- Generally, as we train the model for longer, the BLEU score should be increasing (we will include a chart to show this).\n",
        "\n",
        "> Discuss what happened\n",
        "\n",
        "## Desired Goals\n",
        "\n",
        "- Achieve results of translation that is not simply mappings between the two language vocabularies but rather encompasses the context and attention mappings of the whole sentence. This could be achieved by inputting examples where gendered noun and adjective would be correctly translated (i.e., the Transformer can differentiate between genders).\n",
        "\n",
        "  - \"I am a tall man\" vs. \"I am a tall woman\"\n",
        "\n",
        "> Discuss what happened\n",
        "\n",
        "- Have consistency across languages when training under the same model in terms of BLEU score. Testing with another language dataset (most likely English to French) and have a similar BLEU score performance under the same training settings and time.\n",
        "\n",
        "> Discuss what happened\n",
        "\n",
        "## Stretch Goals\n",
        "\n",
        "- Based on the findings from the paper \"Reformer: The Efficient Transformer\", we would try to implement and quantify the impact of the suggested changes from that paper compared to the Transformer.\n",
        "\n",
        "> Discuss what happened\n",
        "\n",
        "- Try pretraining the model on English-to-French, then fine tune the model to translate English-to-Spanish (the motivation is that since both French and Spanish are Romance languages, the pretrained model could have already learned important parts of the mapping from English to any Romance language).\n",
        "\n",
        "> Discuss what happened\n",
        "\n",
        "## Other Challenges\n",
        "\n",
        "- Challenge 1\n",
        "  - Why was it important?\n",
        "  - What was so challenging/interesting about this?\n",
        "- Challenge 2\n",
        "  - Why was it important?\n",
        "  - What was so challenging/interesting about this?\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Code and Documentation\n",
        "\n",
        "## `data.py`\n",
        "\n",
        "This contains classes that processes our data in the format we desire. This is relevant as it makes using our data easier.\n",
        "\n",
        "## `decoder.py`\n",
        "\n",
        "This contains the decoder portion of the Transformer model as well as each individual decoder layer. This is what processes the output of the encoder and outputs the translation.\n",
        "\n",
        "## `encoder.py`\n",
        "\n",
        "This contains the encoder portion of the Transformer model as well as each individual encoder layer. This is what processes the input and captures the information from the input.\n",
        "\n",
        "## `multihead_attention.py`\n",
        "\n",
        "This contains the Multi-Head Attention portion of the Transformer model, present in both the encoder and the decoder. This is what makes it possible to capture information from an input.\n",
        "\n",
        "## `optimizer.py`\n",
        "\n",
        "This contains a specific version of the Adam optimizer, Dynamic LR Adam. This is relevant as this is what the paper used, and it is respectively used to optimize our model.\n",
        "\n",
        "## `position_wise_feed_forward.py`\n",
        "\n",
        "What does this contain and why it's relevant?\n",
        "\n",
        "## `positional_encoder.py`\n",
        "\n",
        "This contains our `PositionalEncoding` class. This is required as this is how the Transformer understands the positions of the inputs, something that is naturally included in an LSTM.\n",
        "\n",
        "## `transformer_runner.py`\n",
        "\n",
        "This contains the runner of the model. This is how the model is run by using the trainer in `transformer_trainer.py`.\n",
        "\n",
        "## `transformer_trainer.py`\n",
        "\n",
        "This contains the trainer of the model. This is how the model gets trained.\n",
        "\n",
        "## `transformer.py`\n",
        "\n",
        "This contains the Transformer model, which pretty much puts everything together. This is the end goal of the project, which is creating a Transformer model.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reflections\n",
        "\n",
        "## What was interesting?\n",
        "\n",
        "## What was difficult?\n",
        "\n",
        "## What's left to do?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "attention",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
